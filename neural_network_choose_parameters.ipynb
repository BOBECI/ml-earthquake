{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "/home/emreunel/anaconda3/envs/ass3/lib/python3.7/site-packages/sklearn/utils/__init__.py:4: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from collections import Sequence\n",
      "/home/emreunel/anaconda3/envs/ass3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:16: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from collections import Mapping, namedtuple, defaultdict, Sequence\n",
      "/home/emreunel/anaconda3/envs/ass3/lib/python3.7/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "#importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.combine import SMOTETomek\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from torch.optim.lr_scheduler import MultiStepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "dtype = torch.cuda.FloatTensor # Uncomment this to run on GPU"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "(762094, 44)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 3
    }
   ],
   "source": [
    "# load dataset\n",
    "data = pd.read_csv(\"dataset/preprocessed.csv\")\n",
    "data = data.drop(data[data.target == -1].index)\n",
    "data.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Separate input features and target\n",
    "targets = data.target\n",
    "targets -= 1\n",
    "targets = targets.to_numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "features = data.drop('target', axis=1)\n",
    "features = features.to_numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# split test part\n",
    "X_trainAndVal, X_test, y_trainAndVal, y_test = train_test_split(features, targets, test_size = 0.25, random_state = 0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# split train and validation part\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainAndVal, y_trainAndVal, test_size = 0.25, random_state = 0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "/home/emreunel/anaconda3/envs/ass3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "#Scale data\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_val = sc.transform(X_val)\n",
    "X_test = sc.transform(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([1.9381, 1.7462, 1.1196, 0.8281, 0.5523], dtype=torch.float64)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 9
    }
   ],
   "source": [
    "# calculate weight for targets\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "class_weights = compute_class_weight('balanced', np.unique(y_train), y_train)\n",
    "class_weights = torch.from_numpy(class_weights)\n",
    "class_weights"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# data load\n",
    "class datasetLoad(Dataset):\n",
    "    def __init__(self, features,labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    def __getitem__(self, index):\n",
    "        return self.features[index], self.labels[index]\n",
    "X_train = datasetLoad(X_train, y_train)\n",
    "X_val = datasetLoad(X_val, y_val)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# focal loss\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, focusing_param = 2, balance_param=0.5):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.focusing_param = focusing_param\n",
    "        self.balance_param = balance_param\n",
    "    def forward(self, output, target):\n",
    "        cross_entropy = F.cross_entropy(output, target)\n",
    "        cross_entropy_log = torch.log(cross_entropy)\n",
    "        logpt = - F.cross_entropy(output, target)\n",
    "        pt    = torch.exp(logpt)\n",
    "        focal_loss = -((1 - pt) ** self.focusing_param) * logpt\n",
    "        balanced_focal_loss = self.balance_param * focal_loss\n",
    "        return balanced_focal_loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# network\n",
    "class neuralNetwork(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden1_dim, hidden2_dim, output_dim, dropout_p):\n",
    "        super(neuralNetwork, self).__init__()\n",
    "        \n",
    "        self.hidden1 = nn.Linear(input_dim, hidden1_dim, bias=True) \n",
    "        torch.nn.init.xavier_uniform(self.hidden1.weight)\n",
    "        self.bnhidden1 = nn.BatchNorm1d(hidden1_dim)\n",
    "        \n",
    "        self.hidden2 = nn.Linear(hidden1_dim, hidden2_dim, bias=True) \n",
    "        torch.nn.init.xavier_uniform(self.hidden2.weight)\n",
    "        self.bnhidden2 = nn.BatchNorm1d(hidden2_dim)\n",
    "        \n",
    "        self.output = nn.Linear(hidden2_dim, output_dim, bias=True)\n",
    "        torch.nn.init.xavier_uniform(self.output.weight)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.hidden1(x)   \n",
    "        x = self.dropout(x)\n",
    "        x = self.bnhidden1(x)        \n",
    "        x = F.leaky_relu_(x, negative_slope=0.01)\n",
    "        \n",
    "        x = self.hidden2(x)        \n",
    "        x = self.dropout(x)\n",
    "        x = self.bnhidden2(x)\n",
    "        x = F.leaky_relu_(x, negative_slope=0.01)\n",
    "        \n",
    "        \n",
    "        outputs = self.output(x)\n",
    "        return outputs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# network settings\n",
    "import sys\n",
    "epsilon = sys.float_info.epsilon\n",
    "\n",
    "batch_size = 100000\n",
    "epochs = 20\n",
    "input_dim = 43\n",
    "output_dim = 5\n",
    "lr = 0.05\n",
    "momentum_val = 0.9\n",
    "weight_decay_val = 0.00001\n",
    "\n",
    "gamma_val = 0.5\n",
    "prob = 0.1\n",
    "\n",
    "\n",
    "old_loss = 1 / epsilon\n",
    "cur_loss = 0.0\n",
    "best_loss = 1 / epsilon\n",
    "\n",
    "loss_dicrease_count = 0\n",
    "loss_dicrease_limit = 3\n",
    "loss_dicrease_threshold = 0.001\n",
    "\n",
    "early_stop_epoch = 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# choose new parameters to test on tensorboard\n",
    "parameters = dict(\n",
    "    lr = [.01,0.0001],\n",
    "    batch_size = [100, 10000],\n",
    "    weight_decay_val = [0.001, 0.0000001]\n",
    ")\n",
    "param_values = [v for v in parameters.values()]\n",
    "param_values\n",
    "from itertools import product"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      " batch_size=100 lr=0.01 weight_decay_val=0.001 \n",
      "{\n",
      "Epoch: 19. Train Loss: 0.32050237584105734. \n",
      "Epoch: 19. Validation Loss: 0.32143777769457804. Validation Accuracy: 40.191424513661616.\n",
      "}\n",
      " batch_size=100 lr=0.01 weight_decay_val=1e-07 \n",
      "{\n",
      "Epoch: 19. Train Loss: 0.31495055365123087. \n",
      "Epoch: 19. Validation Loss: 0.31650906437297566. Validation Accuracy: 40.91543180443704.\n",
      "}\n",
      " batch_size=10000 lr=0.01 weight_decay_val=0.001 \n",
      "{\n",
      "Epoch: 19. Train Loss: 0.35289568540661836. \n",
      "Epoch: 19. Validation Loss: 0.35395276347796123. Validation Accuracy: 32.19787475043081.\n",
      "}\n",
      " batch_size=10000 lr=0.01 weight_decay_val=1e-07 \n",
      "{\n",
      "Epoch: 19. Train Loss: 0.3609792749549067. \n",
      "Epoch: 19. Validation Loss: 0.36233711838722227. Validation Accuracy: 31.302949431945446.\n",
      "}\n",
      " batch_size=100 lr=0.0001 weight_decay_val=0.001 \n",
      "{\n",
      "Epoch: 19. Train Loss: 0.36363318384541815. \n",
      "Epoch: 19. Validation Loss: 0.3644615206333038. Validation Accuracy: 30.779404280016596.\n",
      "}\n",
      " batch_size=100 lr=0.0001 weight_decay_val=1e-07 \n",
      "{\n",
      "Epoch: 19. Train Loss: 0.3596375495862761. \n",
      "Epoch: 19. Validation Loss: 0.3613176236751615. Validation Accuracy: 30.769993097710294.\n",
      "}\n",
      " batch_size=10000 lr=0.0001 weight_decay_val=0.001 \n",
      "{\n",
      "Epoch: 19. Train Loss: 0.5544124855551609. \n",
      "Epoch: 19. Validation Loss: 0.5539817134539287. Validation Accuracy: 27.303291020276916.\n",
      "}\n",
      " batch_size=10000 lr=0.0001 weight_decay_val=1e-07 \n",
      "{\n",
      "Epoch: 19. Train Loss: 0.5568287039912024. \n",
      "Epoch: 19. Validation Loss: 0.5559477210044861. Validation Accuracy: 21.54556138134491.\n",
      "}\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": [
      "/home/emreunel/anaconda3/envs/ass3/lib/python3.7/site-packages/ipykernel_launcher.py:7: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  import sys\n",
      "/home/emreunel/anaconda3/envs/ass3/lib/python3.7/site-packages/ipykernel_launcher.py:11: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/home/emreunel/anaconda3/envs/ass3/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  from ipykernel import kernelapp as app\n",
      "/home/emreunel/anaconda3/envs/ass3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/emreunel/anaconda3/envs/ass3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# test parameters\n",
    "import datetime\n",
    "a = datetime.datetime.now().replace(microsecond=0)\n",
    "for lr, batch_size, weight_decay_val in product(*param_values): \n",
    "    comment = f' batch_size={batch_size} lr={lr} weight_decay_val={weight_decay_val} '\n",
    "    print(comment)\n",
    "    model = neuralNetwork(input_dim, 20, 10, output_dim, prob)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(dataset = X_train, batch_size = batch_size, shuffle=True, num_workers = 1)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset = X_val, batch_size = batch_size, shuffle=True, num_workers = 1)\n",
    "\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr = lr, momentum = momentum_val, weight_decay = weight_decay_val)\n",
    "    scheduler = MultiStepLR(optimizer, milestones=[30, 60, 80], gamma = gamma_val)\n",
    "    criterion = FocalLoss()\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "\n",
    "    feature_train, label_train = next(iter(train_loader))\n",
    "    if torch.cuda.is_available():\n",
    "        feature_train = feature_train.cuda()\n",
    "    grid_train = torchvision.utils.make_grid(feature_train)\n",
    "    \n",
    "    tb = SummaryWriter(comment=comment)\n",
    "    \n",
    "    tb.add_image(\"features\", grid_train)\n",
    "    tb.add_graph(model, feature_train.float())\n",
    "   \n",
    "    train_loss = []\n",
    "    validation_loss = []\n",
    "    for epoch in range(epochs):\n",
    "        train_loss_val = 0.0\n",
    "        train_counter = 0\n",
    "        validation_loss_val = 0.0\n",
    "        val_counter = 0\n",
    "        accuracy = 0.0\n",
    "        for i, (features_train, labels_train) in enumerate(train_loader):\n",
    "            features_train = Variable(features_train)\n",
    "            labels_train = Variable(labels_train)\n",
    "            if torch.cuda.is_available():\n",
    "                features_train = features_train.cuda()\n",
    "                labels_train = labels_train.cuda()        \n",
    "            optimizer.zero_grad()\n",
    "            outputs_train = model(features_train.float())\n",
    "            loss_train = criterion(outputs_train.float(), labels_train)\n",
    "            loss_train.backward()\n",
    "            optimizer.step()\n",
    "            train_loss_val += loss_train.item()\n",
    "            train_counter += 1\n",
    "            del features_train\n",
    "            del labels_train        \n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        train_loss_val /= train_counter    \n",
    "        \n",
    "        for i, (features_val, labels_val) in enumerate( val_loader):\n",
    "            features_val = Variable(features_val)\n",
    "            labels_val = Variable(labels_val)\n",
    "            if torch.cuda.is_available():\n",
    "                features_val = features_val.cuda()\n",
    "                labels_val = labels_val.cuda() \n",
    "            with torch.no_grad():\n",
    "                outputs_val = model(features_val.float())\n",
    "            loss_val = criterion(outputs_val.float(), labels_val)\n",
    "            validation_loss_val += loss_val.item()\n",
    "            _, predicted = torch.max(outputs_val.data, 1)\n",
    "            # for gpu, bring the predicted and labels back to cpu fro python operations to work\n",
    "            accuracy += f1_score(labels_val.cpu(), predicted.cpu(), average = 'weighted') * 100\n",
    "            val_counter += 1\n",
    "            del features_val\n",
    "            del labels_val          \n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        validation_loss_val /= val_counter\n",
    "        accuracy /=  val_counter\n",
    "        \n",
    "        cur_loss = validation_loss_val\n",
    "        \n",
    "        if(cur_loss < best_loss):\n",
    "            torch.save(model.state_dict(), 'weights_only.pth')\n",
    "            early_stop_epoch = epoch\n",
    "            best_loss = cur_loss\n",
    "            \n",
    "        if(cur_loss > old_loss + loss_dicrease_threshold):\n",
    "            loss_dicrease_count += 1\n",
    "            \n",
    "        if(cur_loss + loss_dicrease_threshold < old_loss):\n",
    "            loss_dicrease_count = 0\n",
    "            \n",
    "        if(loss_dicrease_count == loss_dicrease_limit):\n",
    "            print(\"--------------------\\n\\n\\nYOU NEED STOP\\n\\n\\n\\n----------\")\n",
    "            break\n",
    "        \n",
    "        old_loss = cur_loss\n",
    "           \n",
    "        scheduler.step()\n",
    "           \n",
    "        train_loss.append(train_loss_val)\n",
    "        validation_loss.append(validation_loss_val)\n",
    "        if(epoch == epochs - 1):\n",
    "            print(\"{\")\n",
    "            print(\"Epoch: {}. Train Loss: {}. \".format(epoch, train_loss_val))\n",
    "            print(\"Epoch: {}. Validation Loss: {}. Validation Accuracy: {}.\".format(epoch, validation_loss_val, accuracy))\n",
    "            print(\"}\")\n",
    "            \n",
    "        tb.add_scalar(\"Train Loss \", train_loss_val, epoch)\n",
    "        tb.add_scalar(\"Validation Loss \", validation_loss_val, epoch)\n",
    "        tb.add_scalar(\"Validation Accur \", accuracy, epoch) \n",
    "            \n",
    "        for name, weight in model.named_parameters():\n",
    "            tb.add_histogram(name, weight, epoch)\n",
    "            tb.add_histogram(f'{name}.grad', weight.grad, epoch)     \n",
    "    tb.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "0:30:50\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# calculate time difference and give warning when the epochs finish\n",
    "b = datetime.datetime.now().replace(microsecond=0)\n",
    "print(b-a)\n",
    "import os,time\n",
    "counter = 0\n",
    "while(counter < 1):\n",
    "    os.system('spd-say \"your program has finished\"')\n",
    "    time.sleep(3)\n",
    "    counter += 1\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}